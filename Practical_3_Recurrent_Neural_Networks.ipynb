{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practical 3: Recurrent Neural Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "xiUrsPAI36M-"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "9jDBz0IbW3Xy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Practical 3: Recurrent Neural Networks (RNNs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-0F3Ao8BKa0g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Feedforward models (eg deep MLPs and ConvNets) map fixed-size input-data (vectors of a fixed dimensionality) to their output labels. They're very powerful and have been successfully used for many tasks. However, a lot of data is not in the form of fixed-size vectors, but exists in the form of **sequences**. Language is one good example, where sentences are sequences of words. In some way, almost any data types can be considered as a sequence (for instance an image consists of a sequence of pixels, speech a sequence of phonemes, and so forth). \n",
        "\n",
        "Recurrent neural networks (**RNNs**) were designed to be able to handle sequential data, and in this practical we will take a closer look at RNNs and then build a model that can generate English sentences in the style of Shakespeare!"
      ]
    },
    {
      "metadata": {
        "id": "otAAvBVFSZy8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "* Understand how RNNs model sequential data.\n",
        "* Understand how the vanilla RNN is a generalization of feedforward models to incorporate sequential dependencies.\n",
        "* Understand the issues involved when training RNNs.\n",
        "* Know how to implement an RNN for time-series estimation (**regression**) and an RNN language model (character-level **classification**) in Tensorflow using Keras."
      ]
    },
    {
      "metadata": {
        "id": "gfKcEFUxa--9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Imports\n"
      ]
    },
    {
      "metadata": {
        "id": "h8glXxcyew17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c102ff05-fa6e-486d-ac0b-6fe4d919e701"
      },
      "cell_type": "code",
      "source": [
        "#@title Imports (RUN ME!) { display-mode: \"form\" }\n",
        "\n",
        "#!pip -q install pydot_ng\n",
        "#!pip -q install graphviz\n",
        "#!apt install graphviz > /dev/null\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import random\n",
        "import ssl\n",
        "import sys\n",
        "import urllib2\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print('Running TensorFlow version %s' % (tf.__version__))\n",
        "try:\n",
        "  tf.enable_eager_execution()\n",
        "  print('Eager mode activated.')\n",
        "except ValueError:\n",
        "  print('Already running in Eager mode')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running TensorFlow version 1.10.1\n",
            "Eager mode activated.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yVL1OwL7aH8c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##From Feedforward to Recurrent Models\n",
        "\n",
        "### Intuition\n",
        "RNNs generalize feedforward networks (FFNs) to be able to work with sequential data. FFNs take an input (e.g. an image) and immediately produce an output (e.g. a digit class), something like this:"
      ]
    },
    {
      "metadata": {
        "id": "NqZsIaRU6-WK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ffn_forward(x, W_xh, W_ho, b_hid, b_out):\n",
        " \n",
        "    # Compute activations on the hidden layer.\n",
        "    hidden_layer = act_fn(np.dot(W_xh, x) + b_hid)\n",
        "    \n",
        "    # Compute the (linear) output layer activations.     \n",
        "    output = np.dot(W_ho, hidden_layer) + b_out\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kb3Tjms06_XL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE**: You don't have to run this cell, it's just shown to illustrate the point.\n",
        "\n",
        "RNNs, on the other hand, consider the data sequentially and remember what they have seen in the past in order to make new predictions about the future observations, something like this:\n"
      ]
    },
    {
      "metadata": {
        "id": "ACx_wHGB7AWc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rnn_forward(data_sequence, initial_state):\n",
        "\n",
        "    state = initial_state  # Reused at every time-step\n",
        "    all_states, all_ys = [state], []  # Used to save all states and predictions\n",
        "\n",
        "    for x, y in data_sequence:\n",
        "      \n",
        "      # recurrent_fn() takes the current input and the previous state and produces a new state\n",
        "      new_state, y_pred = recurrent_fn(x, state)\n",
        "      \n",
        "      all_states.append(new_state)\n",
        "      all_ys.append(y_pred)\n",
        "      \n",
        "      # Update state for the next time-step\n",
        "      state = new_state\n",
        "\n",
        "    return all_states, all_ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Crh_ViE7Ave",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To understand the distinction between FFNs and RNNs, imagine we want to label words as the part-of-speech categories that they belong to: E.g. for the input sentence \"I want a duck\" and \"He had to duck\", we want our model to predict that duck is a `Noun` in the first sentence and a `Verb` in the second. To do this successfully, the model needs to be aware of the surrounding context. However, if we feed a FFN model only one word at a time, how could it know the difference? If we want to feed it all the words at once, how do we deal with the fact that sentences are of different lengths?\n",
        "\n",
        "RNNs solve this issue by processing the sentence word-by-word, and maintaining an internal **state** summarizing what it has seen so far. This applies not only to words, but also to phonemes in speech, or even, as we will see, elements of a time-series.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zUqww79L6Ot-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Unrolling the network\n",
        "\n",
        "Imagine we are trying to classify sequences $X = (x_1, x_2, \\ldots, x_N)$ into labels $y$ (for now, let's keep it abstract). After running the `rnn_forward()` function of our RNN defined above on $X$, we would have a list of internal states and outputs of the model at each sequence position. This process is called **unrolling in time**, because you can think of it as unrolling the *computations* defined by the RNN loop over the inputs at each position of the sequence.  RNNs are often used to model **time series data** (which we will do in this practical), and therefore these positions are referred to as **time-steps**, and hence we call this process \"unrolling over time\".\n",
        "\n",
        "**TODO(sgouws)**: include graph to display this, e.g. http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg\n",
        "\n",
        "> **We can therefore think of an RNN as a composition of identical feedforward neural networks (with replicated/tied weights), one for each moment or step in time. **\n",
        "\n",
        "These feedforward functions that make up the RNN(i.e. our `recurrent_fn` above) are typically referred to as **cells**, and the only restriction on its API is that the cell function needs to be a differentiable function that can map an input and a state vector to an output and a new state vector. What we have shown above is called the **vanilla RNN**, but there are many more possibilities. One of the most popular variants is called the **Long Short-Term Memory (LSTM)** cell, which we'll use later to create our Shakespeare language model.\n",
        "\n",
        "### Putting this together \n",
        "\n",
        "In the feedforward models we've seen before, the input $x$ is mapped to an intermediate hidden layer $h$ as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "  h = \\sigma(\\underbrace{W_{xh}x}_\\text{current input (per-example)} + b)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\sigma$ is some non-linear activation function like ReLU or tanh.  We can then make a prediction $\\hat{y} = \\sigma(W_{hy}h + b)$ based on $h$, or we can add another layer, etc.\n",
        "\n",
        "RNNs generalize this idea to a sequence of inputs $X = {x_1, x_2, ...}$ by maintaining a sequence of state vectors $h_t$, one for every time-step $t$, as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "  h_t = \\sigma(\\underbrace{W_{hh}h_{t-1}}_\\text{previous state} + \\underbrace{W_{xh}x_t}_\\text{current input (per time-step)} + b)\n",
        "\\end{equation}\n",
        "\n",
        "We can again use each $h_t$ to predict an output for that time-step $y_t = \\sigma(W_{hy}h_t + b)$ (e.g. the part-of-speech of each word in a sentence), or we can just make one final prediction at the end using $h_T$ (e.g. the topic of a document processed as a sequence of words).\n",
        "\n",
        "**NOTE**: The weight subscript $W_{xz}$ is used to indicate a mapping from layer $x$ to layer $z$.\n",
        "\n",
        "**QUESTIONS**\n",
        "* How are FFNs and RNNs **similar**?\n",
        "* How are they **different**?\n",
        "* Why do we call RNNs \"recurrent\"?"
      ]
    },
    {
      "metadata": {
        "id": "9wuobtH3aB59",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Modeling General Time-Series\n",
        "\n",
        "We will train an RNN to model a time-series as a first step. A **time-series** is a series of data-points ordered over discrete time-steps. Examples include the hourly temperature of Stellenbosch over a month or a year, the market price of some asset (like a company's stock) over time, and so forth. We will generate a **sinusoidal time-series** (with or without noise) as a toy example, and then train a tiny RNN model with only 5 parameters on this data."
      ]
    },
    {
      "metadata": {
        "id": "oAQIXUL-oje2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Create some artificial data"
      ]
    },
    {
      "metadata": {
        "id": "412j4v-RIfYR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "b1b1d64f-630e-411e-f973-f7e7554f044e"
      },
      "cell_type": "code",
      "source": [
        "#@title Create sinusoidal data {run: \"auto\"}\n",
        "steps_per_cycle = 20 #@param { type: \"slider\", min:1, max:100, step:1 }\n",
        "number_of_cycles = 176 #@param { type: \"slider\", min:1, max:1000, step:1 }\n",
        "noise_factor = 0.1 #@param { type: \"slider\", min:0, max:1, step:0.1 }\n",
        "plot_num_cycles = 23 #@param { type: \"slider\", min:1, max:50, step:1 }\n",
        "\n",
        "seq_len = steps_per_cycle * number_of_cycles\n",
        "t = np.arange(seq_len)\n",
        "sin_t_noisy = np.sin(2 * np.pi / steps_per_cycle * t + noise_factor * np.random.uniform(-1.0, +1.0, seq_len))\n",
        "sin_t_clean = np.sin(2 * np.pi / steps_per_cycle * t)\n",
        "\n",
        "upto = plot_num_cycles * steps_per_cycle\n",
        "fig = plt.figure(figsize=(15,3))\n",
        "plt.plot(t[:upto], sin_t_noise[:upto])\n",
        "plt.title(\"Showing first {} cycles.\".format(plot_num_cycles))\n",
        "plt.show()\n",
        "\n",
        "#both = np.column_stack((t, sin_t_noisy))\n",
        "#print(\"both.shape = {}\".format(both.shape))\n",
        "\n",
        "#print(\"both[:steps_per_cycle, :steps_per_cycle]\")\n",
        "#print(both[:steps_per_cycle,:steps_per_cycle])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA24AAADRCAYAAAC0JeFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9c91640a10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Eaypg9gKos21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TASK**: Adjust the parameters above to generate data with different properties.\n",
        "\n",
        "Now we pack the data into train and test batches. Note that while RNNs can in theory learn the dependencies across all inputs received so far (using an algorithm called **backpropagation through time**, or BPTT; see the Aside box below), in practice they are trained using an algorithm called **truncated BPTT** where we truncate the inputs to only the last $T$ symbols (this is the `truncated_seq_len` variable below).\n",
        "\n",
        "**QUESTION**: What are the pros and cons of truncating the training data in this way?"
      ]
    },
    {
      "metadata": {
        "id": "RtwMLLCNorw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "cellView": "both",
        "outputId": "b21cacaf-acbb-4d67-dc56-a4ac6c2d1339"
      },
      "cell_type": "code",
      "source": [
        "#@title Pack truncated sequence data {run: \"auto\"}\n",
        "\n",
        "def pack_truncated_data(data, num_prev = 100):  \n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - num_prev):\n",
        "      X.append(data[i : i + num_prev])\n",
        "      Y.append(data[i + num_prev])\n",
        "    # NOTE: Keras expects input data in the shape (batch_size, truncated_seq_len, input_dim)\n",
        "    # We have only one real-valued number per time-step, so we therefore expand \n",
        "    # the last dimension from (batch_size, truncated_seq_len) to \n",
        "    # (batch_size, truncated_seq_len, 1).\n",
        "    X, Y = np.array(X)[:,:,np.newaxis], np.array(Y)[:,np.newaxis]\n",
        "    return X, Y\n",
        "\n",
        "# We only consider this many previous data points\n",
        "truncated_seq_len = 2 #@param { type: \"slider\", min:1, max:10, step:1 }\n",
        "test_split = 0.25  # Fraction of total data to keep out as test data\n",
        "\n",
        "# We use only the sin(t) values, and discard the time values\n",
        "data = sin_t_noisy\n",
        "data_len = data.shape[0]\n",
        "num_train = int(data_len * (1 - test_split))\n",
        "\n",
        "train_data = data[:num_train]\n",
        "test_data = data[num_train:]\n",
        "\n",
        "X_train, y_train = pack_truncated_data(train_data, num_prev=truncated_seq_len)\n",
        "X_test, y_test = pack_truncated_data(test_data, num_prev=truncated_seq_len)  \n",
        "\n",
        "print(\"Generated training/test data with shapes\\nX_train: {}, y_train: {}\\nX_test: {}, y_test: {}. \".format(\n",
        "    X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated training/test data with shapes\n",
            "X_train: (2638, 2, 1), y_train: (2638, 1)\n",
            "X_test: (878, 2, 1), y_test: (878, 1). \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vgVxLBp8CaN6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE**: We reshape the training data into (batch_size, truncated_seq_len, 1) and (batch_size, 1) arrays."
      ]
    },
    {
      "metadata": {
        "id": "xiUrsPAI36M-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Intermediate Aside: (Truncated) Backpropagation-through-Time and Vanishing and Exploding Gradients"
      ]
    },
    {
      "metadata": {
        "id": "SCJlN3O11pr1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNs model sequential data, and are designed to capture how ***outputs*** at the current time step are influenced by the ***inputs*** that came before them. This is referred to as **long-range dependencies**. At a high level, this allows the model to remember what it has seen so far in order to better contextualize what it is seeing at the moment (think about how knowing the context of the sentence or conversation can sometimes help one to better figure out the intended meaning of a misheard word or ambiguous statement). It is what makes these models so powerful, but it is also what makes them so hard to train!\n",
        "\n",
        "The most well-known algorithm for training RNNs is called **back-propagation through time (BPTT**; there are other algorithms). BPTT conceptually amounts to unrolling the computations of the RNN over time, computing the errors, and backpropagating the gradients through the unrolled graph structure.  Ideally we want to unroll the graph up to the maximum sequence length, however in practice, since sequence lengths vary and memory is limited, we only end up unrolling sequences up to some length $T$. This is called **truncated BPTT**, and is the most used variant of BPTT.\n",
        "\n",
        "At a high level, there are two main issues when using (truncated) BPTT to train RNNs:\n",
        "\n",
        "* Having shared (\"tied\") recurrent weights ($W_{hh}$) mean that **the gradient on these weights at some time step $t$ depends on all time steps up to time-step $T$**, the length of the full (truncated) sequence. This also leads to the **vanishing/exploding gradients** problem.\n",
        "\n",
        "* **Memory usage grows linearly with the total number of steps $T$ that we unroll for**, because we need to save/cache the activations at each time-step (look at the Python code above to convince yourself of this).  This matters computationally, since memory is a limited resource. It also matters statistically, because it puts a limit on the types of dependencies the model is exposed to, and hence that it could learn.\n",
        "\n",
        "**NOTE**: Think about that last statement and make sure you understand those 2 points.\n",
        "\n",
        "BPTT is very similar to the standard back-propagation algorithm. Key to understanding the BPTT algorithm is to realize that gradients on the non-recurrent weights (weights of a per time-step classifier that tries to predict the part-of-speech tag for each word for example) and recurrent weights (that transform $h_{t-1}$ into $h_t$) are computed differently:\n",
        "\n",
        "* The gradients of **non-recurrent weights** ($W_{hy}$) depend only on the error at that time-step, $E_t$.\n",
        "* The gradients of **recurrent weights** ($W_{hh}$) depend on all previous time-steps up to maximum length $T$.\n",
        "\n",
        "The first point is fairly intuitive: predictions at time-step $t$ is related to the loss of that particular prediction. \n",
        "\n",
        "The second point will be explained in more detail in the lectures (see also [this great blog post](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)), but briefly, this can be summarized in these equations:\n",
        "\n",
        "1. The **current** state is a function of the **previous** state and the current input: $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t)$\n",
        "2. The gradient of the loss $E_t$ at time $t$ on $W_{hh}$ is a function of the current hidden state and model predictions $\\hat{y}_t$ at time t: \n",
        "$\\frac{\\partial E_t}{\\partial W_{hh}} = \\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial W_{hh}}$\n",
        "3. Substituting (1) into (2) results in a **sum over all previous time-steps**:\n",
        "$\\frac{\\partial E_t}{\\partial W_{hh}} = \\sum\\limits_{k=0}^{t} \\underbrace{\\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial h_k}\\frac{\\partial h_k}{\\partial W_{hh}}}_\\text{product of gradient terms}$\n",
        "\n",
        "Because of this **repeated multiplicative interaction**, as the sequence length $t$ gets longer, the gradients themselves can get diminishingly small (**vanish**) or grow too large and result in numeric overflow (**explode**). This has been shown to be related to the norms of the recurrent weight matrices being less than or equal to 1. Intuitively, it works very similar to how multiplying a small number $v<1.0$ with itself repeatedly can quickly go to zero, or conversely, a large number $v>1.0$ could quickly go to infinity; only this is for matrices.\n"
      ]
    },
    {
      "metadata": {
        "id": "-o3qjqeZpLjN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Build a tiny RNN in Keras\n",
        "\n",
        "Building an RNN in Keras is quite simple. We simply chain the layers together as follows:"
      ]
    },
    {
      "metadata": {
        "id": "mEobTD6spOWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def define_model(truncated_seq_len):  \n",
        "    \n",
        "    input_dimension = 1\n",
        "    hidden_dimension = 1\n",
        "    output_dimension = 1\n",
        "    \n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.SimpleRNN(\n",
        "        # We need to specify the input_shape *without* leading batch_size (it is inferred)\n",
        "        input_shape=(truncated_seq_len, input_dimension),\n",
        "        units=hidden_dimension, \n",
        "        return_sequences=False,\n",
        "        name='hidden_layer'))\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        output_dimension, \n",
        "        name='output_layer'))\n",
        "\n",
        "    model.compile(loss=\"mean_squared_error\", \n",
        "                  optimizer=tf.train.AdamOptimizer(learning_rate=1e-3))\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JHRscZaQze26",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE**: We're building an RNN for **regression**. We therefore use a linear layer (which outputs real-valued numbers) at the output with the \"*mean_squared_error*\" loss function."
      ]
    },
    {
      "metadata": {
        "id": "ONvBy4AopTrF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "4cff0a47-5688-457a-c071-a5458decf8f6"
      },
      "cell_type": "code",
      "source": [
        "model = define_model(truncated_seq_len = X_train.shape[1])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer (SimpleRNN)     (None, 1)                 3         \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ddb6_04dfZvn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE**: You need to re-run the above cell every time after training to reset the model weights!"
      ]
    },
    {
      "metadata": {
        "id": "hD94X5iQc8Jg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Train the tiny RNN\n",
        "Now let's train the model. This may take a few minutes (it takes much longer if you increase `truncated_seq_len`). Set `verbose=1` **before** you run the cell to see the intermediate output as the model is training. Set it to 0 if you don't want any output."
      ]
    },
    {
      "metadata": {
        "id": "xvahCyhk7Wvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "''' SOLUTION TO ONE OF TASKS [DELETE]\n",
        "patience = 5\n",
        "train_history = model.fit(X_train, y_train, batch_size=600, epochs=1000, \n",
        "                          verbose=1, validation_split=0.05,\n",
        "                          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1)])\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xumvRz2lrPus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36035
        },
        "outputId": "39973883-59f5-4233-8f02-83f902e0dc35"
      },
      "cell_type": "code",
      "source": [
        "train_history = model.fit(X_train, y_train, batch_size=600, epochs=1000, \n",
        "                          verbose=1, validation_split=0.05)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2506 samples, validate on 132 samples\n",
            "Epoch 1/1000\n",
            "2506/2506 [==============================] - 0s 26us/step - loss: 0.4968 - val_loss: 0.4914\n",
            "Epoch 2/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4965 - val_loss: 0.4911\n",
            "Epoch 3/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4961 - val_loss: 0.4908\n",
            "Epoch 4/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4958 - val_loss: 0.4904\n",
            "Epoch 5/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4953 - val_loss: 0.4898\n",
            "Epoch 6/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.4949 - val_loss: 0.4893\n",
            "Epoch 7/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4944 - val_loss: 0.4887\n",
            "Epoch 8/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4939 - val_loss: 0.4883\n",
            "Epoch 9/1000\n",
            "2506/2506 [==============================] - 0s 13us/step - loss: 0.4933 - val_loss: 0.4877\n",
            "Epoch 10/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4927 - val_loss: 0.4872\n",
            "Epoch 11/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4920 - val_loss: 0.4865\n",
            "Epoch 12/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4913 - val_loss: 0.4856\n",
            "Epoch 13/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4905 - val_loss: 0.4847\n",
            "Epoch 14/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4896 - val_loss: 0.4838\n",
            "Epoch 15/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4887 - val_loss: 0.4828\n",
            "Epoch 16/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4876 - val_loss: 0.4819\n",
            "Epoch 17/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4865 - val_loss: 0.4808\n",
            "Epoch 18/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4853 - val_loss: 0.4797\n",
            "Epoch 19/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.4840 - val_loss: 0.4784\n",
            "Epoch 20/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.4826 - val_loss: 0.4771\n",
            "Epoch 21/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.4811 - val_loss: 0.4756\n",
            "Epoch 22/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.4794 - val_loss: 0.4741\n",
            "Epoch 23/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4777 - val_loss: 0.4725\n",
            "Epoch 24/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.4758 - val_loss: 0.4707\n",
            "Epoch 25/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4738 - val_loss: 0.4689\n",
            "Epoch 26/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4717 - val_loss: 0.4669\n",
            "Epoch 27/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4695 - val_loss: 0.4646\n",
            "Epoch 28/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4671 - val_loss: 0.4622\n",
            "Epoch 29/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4646 - val_loss: 0.4596\n",
            "Epoch 30/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4619 - val_loss: 0.4568\n",
            "Epoch 31/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.4590 - val_loss: 0.4540\n",
            "Epoch 32/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4560 - val_loss: 0.4510\n",
            "Epoch 33/1000\n",
            "2506/2506 [==============================] - 0s 13us/step - loss: 0.4529 - val_loss: 0.4479\n",
            "Epoch 34/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4496 - val_loss: 0.4446\n",
            "Epoch 35/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4461 - val_loss: 0.4412\n",
            "Epoch 36/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4425 - val_loss: 0.4376\n",
            "Epoch 37/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4388 - val_loss: 0.4337\n",
            "Epoch 38/1000\n",
            "2506/2506 [==============================] - 0s 13us/step - loss: 0.4349 - val_loss: 0.4297\n",
            "Epoch 39/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4308 - val_loss: 0.4256\n",
            "Epoch 40/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.4266 - val_loss: 0.4214\n",
            "Epoch 41/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4223 - val_loss: 0.4170\n",
            "Epoch 42/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4178 - val_loss: 0.4127\n",
            "Epoch 43/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.4133 - val_loss: 0.4082\n",
            "Epoch 44/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.4086 - val_loss: 0.4036\n",
            "Epoch 45/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.4038 - val_loss: 0.3988\n",
            "Epoch 46/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.3988 - val_loss: 0.3939\n",
            "Epoch 47/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.3938 - val_loss: 0.3889\n",
            "Epoch 48/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.3888 - val_loss: 0.3839\n",
            "Epoch 49/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.3836 - val_loss: 0.3789\n",
            "Epoch 50/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.3785 - val_loss: 0.3738\n",
            "Epoch 51/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.3732 - val_loss: 0.3686\n",
            "Epoch 52/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.3679 - val_loss: 0.3634\n",
            "Epoch 53/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.3625 - val_loss: 0.3581\n",
            "Epoch 54/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.3571 - val_loss: 0.3526\n",
            "Epoch 55/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.3516 - val_loss: 0.3472\n",
            "Epoch 56/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.3461 - val_loss: 0.3417\n",
            "Epoch 57/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.3406 - val_loss: 0.3362\n",
            "Epoch 58/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.3350 - val_loss: 0.3306\n",
            "Epoch 59/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.3295 - val_loss: 0.3249\n",
            "Epoch 60/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.3238 - val_loss: 0.3192\n",
            "Epoch 61/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.3182 - val_loss: 0.3136\n",
            "Epoch 62/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.3126 - val_loss: 0.3079\n",
            "Epoch 63/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.3069 - val_loss: 0.3022\n",
            "Epoch 64/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.3013 - val_loss: 0.2964\n",
            "Epoch 65/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.2956 - val_loss: 0.2908\n",
            "Epoch 66/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2900 - val_loss: 0.2851\n",
            "Epoch 67/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2844 - val_loss: 0.2794\n",
            "Epoch 68/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2787 - val_loss: 0.2737\n",
            "Epoch 69/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.2729 - val_loss: 0.2679\n",
            "Epoch 70/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.2673 - val_loss: 0.2621\n",
            "Epoch 71/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2615 - val_loss: 0.2563\n",
            "Epoch 72/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2557 - val_loss: 0.2505\n",
            "Epoch 73/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2500 - val_loss: 0.2446\n",
            "Epoch 74/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2442 - val_loss: 0.2388\n",
            "Epoch 75/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.2385 - val_loss: 0.2329\n",
            "Epoch 76/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2328 - val_loss: 0.2271\n",
            "Epoch 77/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.2270 - val_loss: 0.2213\n",
            "Epoch 78/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2212 - val_loss: 0.2155\n",
            "Epoch 79/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.2155 - val_loss: 0.2098\n",
            "Epoch 80/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2099 - val_loss: 0.2041\n",
            "Epoch 81/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.2042 - val_loss: 0.1984\n",
            "Epoch 82/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.1986 - val_loss: 0.1929\n",
            "Epoch 83/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1931 - val_loss: 0.1873\n",
            "Epoch 84/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.1876 - val_loss: 0.1819\n",
            "Epoch 85/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1822 - val_loss: 0.1764\n",
            "Epoch 86/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1768 - val_loss: 0.1710\n",
            "Epoch 87/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.1714 - val_loss: 0.1656\n",
            "Epoch 88/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.1661 - val_loss: 0.1605\n",
            "Epoch 89/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1610 - val_loss: 0.1554\n",
            "Epoch 90/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1559 - val_loss: 0.1505\n",
            "Epoch 91/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1509 - val_loss: 0.1456\n",
            "Epoch 92/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.1461 - val_loss: 0.1409\n",
            "Epoch 93/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.1414 - val_loss: 0.1363\n",
            "Epoch 94/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1368 - val_loss: 0.1317\n",
            "Epoch 95/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1323 - val_loss: 0.1273\n",
            "Epoch 96/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1279 - val_loss: 0.1231\n",
            "Epoch 97/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1237 - val_loss: 0.1190\n",
            "Epoch 98/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.1196 - val_loss: 0.1150\n",
            "Epoch 99/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.1156 - val_loss: 0.1111\n",
            "Epoch 100/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.1117 - val_loss: 0.1074\n",
            "Epoch 101/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.1080 - val_loss: 0.1039\n",
            "Epoch 102/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.1045 - val_loss: 0.1006\n",
            "Epoch 103/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.1011 - val_loss: 0.0973\n",
            "Epoch 104/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0978 - val_loss: 0.0941\n",
            "Epoch 105/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0946 - val_loss: 0.0911\n",
            "Epoch 106/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0916 - val_loss: 0.0882\n",
            "Epoch 107/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0887 - val_loss: 0.0854\n",
            "Epoch 108/1000\n",
            "2506/2506 [==============================] - 0s 14us/step - loss: 0.0859 - val_loss: 0.0828\n",
            "Epoch 109/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0833 - val_loss: 0.0803\n",
            "Epoch 110/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0808 - val_loss: 0.0779\n",
            "Epoch 111/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0783 - val_loss: 0.0757\n",
            "Epoch 112/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0761 - val_loss: 0.0736\n",
            "Epoch 113/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0739 - val_loss: 0.0716\n",
            "Epoch 114/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0719 - val_loss: 0.0697\n",
            "Epoch 115/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0699 - val_loss: 0.0679\n",
            "Epoch 116/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0681 - val_loss: 0.0663\n",
            "Epoch 117/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0663 - val_loss: 0.0646\n",
            "Epoch 118/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0647 - val_loss: 0.0631\n",
            "Epoch 119/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0631 - val_loss: 0.0616\n",
            "Epoch 120/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0617 - val_loss: 0.0603\n",
            "Epoch 121/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0603 - val_loss: 0.0591\n",
            "Epoch 122/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0590 - val_loss: 0.0579\n",
            "Epoch 123/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0578 - val_loss: 0.0567\n",
            "Epoch 124/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0566 - val_loss: 0.0557\n",
            "Epoch 125/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0556 - val_loss: 0.0547\n",
            "Epoch 126/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0545 - val_loss: 0.0538\n",
            "Epoch 127/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0536 - val_loss: 0.0530\n",
            "Epoch 128/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0527 - val_loss: 0.0522\n",
            "Epoch 129/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0519 - val_loss: 0.0515\n",
            "Epoch 130/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0511 - val_loss: 0.0507\n",
            "Epoch 131/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0504 - val_loss: 0.0500\n",
            "Epoch 132/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0497 - val_loss: 0.0494\n",
            "Epoch 133/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0490 - val_loss: 0.0488\n",
            "Epoch 134/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0484 - val_loss: 0.0482\n",
            "Epoch 135/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0478 - val_loss: 0.0477\n",
            "Epoch 136/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0472 - val_loss: 0.0472\n",
            "Epoch 137/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0467 - val_loss: 0.0467\n",
            "Epoch 138/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0462 - val_loss: 0.0463\n",
            "Epoch 139/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0457 - val_loss: 0.0459\n",
            "Epoch 140/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0453 - val_loss: 0.0455\n",
            "Epoch 141/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0448 - val_loss: 0.0451\n",
            "Epoch 142/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0444 - val_loss: 0.0448\n",
            "Epoch 143/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0440 - val_loss: 0.0444\n",
            "Epoch 144/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0437 - val_loss: 0.0441\n",
            "Epoch 145/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0433 - val_loss: 0.0438\n",
            "Epoch 146/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0430 - val_loss: 0.0435\n",
            "Epoch 147/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0427 - val_loss: 0.0431\n",
            "Epoch 148/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0424 - val_loss: 0.0428\n",
            "Epoch 149/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0421 - val_loss: 0.0426\n",
            "Epoch 150/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0418 - val_loss: 0.0423\n",
            "Epoch 151/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0415 - val_loss: 0.0421\n",
            "Epoch 152/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0412 - val_loss: 0.0419\n",
            "Epoch 153/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0410 - val_loss: 0.0416\n",
            "Epoch 154/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0407 - val_loss: 0.0414\n",
            "Epoch 155/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0405 - val_loss: 0.0411\n",
            "Epoch 156/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0402 - val_loss: 0.0409\n",
            "Epoch 157/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0400 - val_loss: 0.0407\n",
            "Epoch 158/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0398 - val_loss: 0.0405\n",
            "Epoch 159/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0395 - val_loss: 0.0403\n",
            "Epoch 160/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0393 - val_loss: 0.0401\n",
            "Epoch 161/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0391 - val_loss: 0.0399\n",
            "Epoch 162/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0389 - val_loss: 0.0397\n",
            "Epoch 163/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0387 - val_loss: 0.0395\n",
            "Epoch 164/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0385 - val_loss: 0.0393\n",
            "Epoch 165/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0382 - val_loss: 0.0391\n",
            "Epoch 166/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0380 - val_loss: 0.0389\n",
            "Epoch 167/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0378 - val_loss: 0.0387\n",
            "Epoch 168/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0376 - val_loss: 0.0385\n",
            "Epoch 169/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0374 - val_loss: 0.0383\n",
            "Epoch 170/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0372 - val_loss: 0.0381\n",
            "Epoch 171/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0370 - val_loss: 0.0379\n",
            "Epoch 172/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0368 - val_loss: 0.0377\n",
            "Epoch 173/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0366 - val_loss: 0.0375\n",
            "Epoch 174/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0364 - val_loss: 0.0373\n",
            "Epoch 175/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0362 - val_loss: 0.0372\n",
            "Epoch 176/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0360 - val_loss: 0.0370\n",
            "Epoch 177/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0358 - val_loss: 0.0368\n",
            "Epoch 178/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0356 - val_loss: 0.0366\n",
            "Epoch 179/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0354 - val_loss: 0.0364\n",
            "Epoch 180/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0352 - val_loss: 0.0362\n",
            "Epoch 181/1000\n",
            "2506/2506 [==============================] - 0s 20us/step - loss: 0.0351 - val_loss: 0.0360\n",
            "Epoch 182/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0349 - val_loss: 0.0359\n",
            "Epoch 183/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0347 - val_loss: 0.0357\n",
            "Epoch 184/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0345 - val_loss: 0.0355\n",
            "Epoch 185/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0343 - val_loss: 0.0353\n",
            "Epoch 186/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0341 - val_loss: 0.0351\n",
            "Epoch 187/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0339 - val_loss: 0.0349\n",
            "Epoch 188/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0337 - val_loss: 0.0347\n",
            "Epoch 189/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0335 - val_loss: 0.0345\n",
            "Epoch 190/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0333 - val_loss: 0.0343\n",
            "Epoch 191/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0331 - val_loss: 0.0342\n",
            "Epoch 192/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0329 - val_loss: 0.0340\n",
            "Epoch 193/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0327 - val_loss: 0.0338\n",
            "Epoch 194/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0325 - val_loss: 0.0336\n",
            "Epoch 195/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0323 - val_loss: 0.0334\n",
            "Epoch 196/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0321 - val_loss: 0.0332\n",
            "Epoch 197/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0320 - val_loss: 0.0330\n",
            "Epoch 198/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0318 - val_loss: 0.0328\n",
            "Epoch 199/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0316 - val_loss: 0.0326\n",
            "Epoch 200/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0314 - val_loss: 0.0324\n",
            "Epoch 201/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0312 - val_loss: 0.0322\n",
            "Epoch 202/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0310 - val_loss: 0.0321\n",
            "Epoch 203/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0308 - val_loss: 0.0319\n",
            "Epoch 204/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0306 - val_loss: 0.0317\n",
            "Epoch 205/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0304 - val_loss: 0.0315\n",
            "Epoch 206/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0302 - val_loss: 0.0314\n",
            "Epoch 207/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0301 - val_loss: 0.0312\n",
            "Epoch 208/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0299 - val_loss: 0.0310\n",
            "Epoch 209/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0297 - val_loss: 0.0308\n",
            "Epoch 210/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0295 - val_loss: 0.0306\n",
            "Epoch 211/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0293 - val_loss: 0.0304\n",
            "Epoch 212/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0291 - val_loss: 0.0302\n",
            "Epoch 213/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0289 - val_loss: 0.0300\n",
            "Epoch 214/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0287 - val_loss: 0.0298\n",
            "Epoch 215/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0286 - val_loss: 0.0296\n",
            "Epoch 216/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0284 - val_loss: 0.0295\n",
            "Epoch 217/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0282 - val_loss: 0.0293\n",
            "Epoch 218/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0280 - val_loss: 0.0291\n",
            "Epoch 219/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0278 - val_loss: 0.0289\n",
            "Epoch 220/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0276 - val_loss: 0.0288\n",
            "Epoch 221/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0275 - val_loss: 0.0286\n",
            "Epoch 222/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0273 - val_loss: 0.0285\n",
            "Epoch 223/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0271 - val_loss: 0.0283\n",
            "Epoch 224/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0269 - val_loss: 0.0281\n",
            "Epoch 225/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0268 - val_loss: 0.0279\n",
            "Epoch 226/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0266 - val_loss: 0.0277\n",
            "Epoch 227/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0264 - val_loss: 0.0275\n",
            "Epoch 228/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0262 - val_loss: 0.0274\n",
            "Epoch 229/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0260 - val_loss: 0.0272\n",
            "Epoch 230/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0259 - val_loss: 0.0270\n",
            "Epoch 231/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0257 - val_loss: 0.0268\n",
            "Epoch 232/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0255 - val_loss: 0.0267\n",
            "Epoch 233/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0253 - val_loss: 0.0265\n",
            "Epoch 234/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0252 - val_loss: 0.0263\n",
            "Epoch 235/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0250 - val_loss: 0.0262\n",
            "Epoch 236/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0248 - val_loss: 0.0260\n",
            "Epoch 237/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0247 - val_loss: 0.0258\n",
            "Epoch 238/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0245 - val_loss: 0.0257\n",
            "Epoch 239/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0243 - val_loss: 0.0255\n",
            "Epoch 240/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0242 - val_loss: 0.0253\n",
            "Epoch 241/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0240 - val_loss: 0.0252\n",
            "Epoch 242/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0238 - val_loss: 0.0250\n",
            "Epoch 243/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0237 - val_loss: 0.0249\n",
            "Epoch 244/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0235 - val_loss: 0.0247\n",
            "Epoch 245/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0233 - val_loss: 0.0246\n",
            "Epoch 246/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0232 - val_loss: 0.0244\n",
            "Epoch 247/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0230 - val_loss: 0.0242\n",
            "Epoch 248/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0229 - val_loss: 0.0241\n",
            "Epoch 249/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0227 - val_loss: 0.0239\n",
            "Epoch 250/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0226 - val_loss: 0.0238\n",
            "Epoch 251/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0224 - val_loss: 0.0236\n",
            "Epoch 252/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0222 - val_loss: 0.0235\n",
            "Epoch 253/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0221 - val_loss: 0.0233\n",
            "Epoch 254/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0219 - val_loss: 0.0232\n",
            "Epoch 255/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0218 - val_loss: 0.0230\n",
            "Epoch 256/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0216 - val_loss: 0.0229\n",
            "Epoch 257/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0215 - val_loss: 0.0227\n",
            "Epoch 258/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0213 - val_loss: 0.0226\n",
            "Epoch 259/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0212 - val_loss: 0.0224\n",
            "Epoch 260/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0210 - val_loss: 0.0223\n",
            "Epoch 261/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0209 - val_loss: 0.0222\n",
            "Epoch 262/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0208 - val_loss: 0.0220\n",
            "Epoch 263/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0206 - val_loss: 0.0219\n",
            "Epoch 264/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0205 - val_loss: 0.0217\n",
            "Epoch 265/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0203 - val_loss: 0.0216\n",
            "Epoch 266/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0202 - val_loss: 0.0215\n",
            "Epoch 267/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0201 - val_loss: 0.0213\n",
            "Epoch 268/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0199 - val_loss: 0.0212\n",
            "Epoch 269/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0198 - val_loss: 0.0211\n",
            "Epoch 270/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0197 - val_loss: 0.0210\n",
            "Epoch 271/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0195 - val_loss: 0.0208\n",
            "Epoch 272/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0194 - val_loss: 0.0207\n",
            "Epoch 273/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0193 - val_loss: 0.0206\n",
            "Epoch 274/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0191 - val_loss: 0.0205\n",
            "Epoch 275/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0190 - val_loss: 0.0203\n",
            "Epoch 276/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0189 - val_loss: 0.0202\n",
            "Epoch 277/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0188 - val_loss: 0.0201\n",
            "Epoch 278/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0186 - val_loss: 0.0200\n",
            "Epoch 279/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0185 - val_loss: 0.0198\n",
            "Epoch 280/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0184 - val_loss: 0.0197\n",
            "Epoch 281/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0183 - val_loss: 0.0196\n",
            "Epoch 282/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0182 - val_loss: 0.0195\n",
            "Epoch 283/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0180 - val_loss: 0.0194\n",
            "Epoch 284/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0179 - val_loss: 0.0193\n",
            "Epoch 285/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0178 - val_loss: 0.0192\n",
            "Epoch 286/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0177 - val_loss: 0.0191\n",
            "Epoch 287/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0176 - val_loss: 0.0190\n",
            "Epoch 288/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0175 - val_loss: 0.0188\n",
            "Epoch 289/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0174 - val_loss: 0.0187\n",
            "Epoch 290/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0173 - val_loss: 0.0186\n",
            "Epoch 291/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0172 - val_loss: 0.0185\n",
            "Epoch 292/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0171 - val_loss: 0.0185\n",
            "Epoch 293/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0170 - val_loss: 0.0184\n",
            "Epoch 294/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0169 - val_loss: 0.0183\n",
            "Epoch 295/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0168 - val_loss: 0.0182\n",
            "Epoch 296/1000\n",
            "2506/2506 [==============================] - 0s 20us/step - loss: 0.0167 - val_loss: 0.0181\n",
            "Epoch 297/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0166 - val_loss: 0.0180\n",
            "Epoch 298/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0165 - val_loss: 0.0179\n",
            "Epoch 299/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0164 - val_loss: 0.0178\n",
            "Epoch 300/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0163 - val_loss: 0.0177\n",
            "Epoch 301/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0162 - val_loss: 0.0176\n",
            "Epoch 302/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0161 - val_loss: 0.0175\n",
            "Epoch 303/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0160 - val_loss: 0.0174\n",
            "Epoch 304/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0159 - val_loss: 0.0173\n",
            "Epoch 305/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0158 - val_loss: 0.0173\n",
            "Epoch 306/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0157 - val_loss: 0.0172\n",
            "Epoch 307/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0156 - val_loss: 0.0171\n",
            "Epoch 308/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0156 - val_loss: 0.0170\n",
            "Epoch 309/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0155 - val_loss: 0.0169\n",
            "Epoch 310/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0154 - val_loss: 0.0169\n",
            "Epoch 311/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0153 - val_loss: 0.0168\n",
            "Epoch 312/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0152 - val_loss: 0.0167\n",
            "Epoch 313/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0152 - val_loss: 0.0166\n",
            "Epoch 314/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0151 - val_loss: 0.0166\n",
            "Epoch 315/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0150 - val_loss: 0.0165\n",
            "Epoch 316/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0149 - val_loss: 0.0164\n",
            "Epoch 317/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0149 - val_loss: 0.0164\n",
            "Epoch 318/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0148 - val_loss: 0.0163\n",
            "Epoch 319/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0147 - val_loss: 0.0162\n",
            "Epoch 320/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0146 - val_loss: 0.0161\n",
            "Epoch 321/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0146 - val_loss: 0.0161\n",
            "Epoch 322/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0145 - val_loss: 0.0160\n",
            "Epoch 323/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0144 - val_loss: 0.0159\n",
            "Epoch 324/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0144 - val_loss: 0.0159\n",
            "Epoch 325/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0143 - val_loss: 0.0158\n",
            "Epoch 326/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0143 - val_loss: 0.0158\n",
            "Epoch 327/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0142 - val_loss: 0.0157\n",
            "Epoch 328/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0141 - val_loss: 0.0157\n",
            "Epoch 329/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0141 - val_loss: 0.0156\n",
            "Epoch 330/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0140 - val_loss: 0.0156\n",
            "Epoch 331/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0140 - val_loss: 0.0155\n",
            "Epoch 332/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0139 - val_loss: 0.0154\n",
            "Epoch 333/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0138 - val_loss: 0.0154\n",
            "Epoch 334/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0138 - val_loss: 0.0153\n",
            "Epoch 335/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0137 - val_loss: 0.0153\n",
            "Epoch 336/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0137 - val_loss: 0.0152\n",
            "Epoch 337/1000\n",
            "2506/2506 [==============================] - 0s 20us/step - loss: 0.0136 - val_loss: 0.0152\n",
            "Epoch 338/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0136 - val_loss: 0.0151\n",
            "Epoch 339/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0135 - val_loss: 0.0151\n",
            "Epoch 340/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0135 - val_loss: 0.0150\n",
            "Epoch 341/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0134 - val_loss: 0.0150\n",
            "Epoch 342/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0134 - val_loss: 0.0149\n",
            "Epoch 343/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0133 - val_loss: 0.0149\n",
            "Epoch 344/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0133 - val_loss: 0.0149\n",
            "Epoch 345/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0132 - val_loss: 0.0148\n",
            "Epoch 346/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0132 - val_loss: 0.0148\n",
            "Epoch 347/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0132 - val_loss: 0.0147\n",
            "Epoch 348/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0131 - val_loss: 0.0147\n",
            "Epoch 349/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0131 - val_loss: 0.0147\n",
            "Epoch 350/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0130 - val_loss: 0.0146\n",
            "Epoch 351/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0130 - val_loss: 0.0146\n",
            "Epoch 352/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0130 - val_loss: 0.0146\n",
            "Epoch 353/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0129 - val_loss: 0.0145\n",
            "Epoch 354/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0129 - val_loss: 0.0145\n",
            "Epoch 355/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0129 - val_loss: 0.0144\n",
            "Epoch 356/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0128 - val_loss: 0.0144\n",
            "Epoch 357/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0128 - val_loss: 0.0144\n",
            "Epoch 358/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0127 - val_loss: 0.0143\n",
            "Epoch 359/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0127 - val_loss: 0.0143\n",
            "Epoch 360/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0127 - val_loss: 0.0143\n",
            "Epoch 361/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0126 - val_loss: 0.0143\n",
            "Epoch 362/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0126 - val_loss: 0.0142\n",
            "Epoch 363/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0126 - val_loss: 0.0142\n",
            "Epoch 364/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0126 - val_loss: 0.0142\n",
            "Epoch 365/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0125 - val_loss: 0.0142\n",
            "Epoch 366/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0125 - val_loss: 0.0141\n",
            "Epoch 367/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0125 - val_loss: 0.0141\n",
            "Epoch 368/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0125 - val_loss: 0.0141\n",
            "Epoch 369/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0124 - val_loss: 0.0141\n",
            "Epoch 370/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0124 - val_loss: 0.0140\n",
            "Epoch 371/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0124 - val_loss: 0.0140\n",
            "Epoch 372/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0124 - val_loss: 0.0140\n",
            "Epoch 373/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0123 - val_loss: 0.0140\n",
            "Epoch 374/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0123 - val_loss: 0.0140\n",
            "Epoch 375/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0123 - val_loss: 0.0139\n",
            "Epoch 376/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0123 - val_loss: 0.0139\n",
            "Epoch 377/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0122 - val_loss: 0.0139\n",
            "Epoch 378/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0122 - val_loss: 0.0139\n",
            "Epoch 379/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0122 - val_loss: 0.0138\n",
            "Epoch 380/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0122 - val_loss: 0.0138\n",
            "Epoch 381/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0122 - val_loss: 0.0138\n",
            "Epoch 382/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0138\n",
            "Epoch 383/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0138\n",
            "Epoch 384/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0138\n",
            "Epoch 385/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0137\n",
            "Epoch 386/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0121 - val_loss: 0.0137\n",
            "Epoch 387/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0121 - val_loss: 0.0137\n",
            "Epoch 388/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0120 - val_loss: 0.0137\n",
            "Epoch 389/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0120 - val_loss: 0.0137\n",
            "Epoch 390/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0137\n",
            "Epoch 391/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0137\n",
            "Epoch 392/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0137\n",
            "Epoch 393/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0120 - val_loss: 0.0136\n",
            "Epoch 394/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0120 - val_loss: 0.0136\n",
            "Epoch 395/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0119 - val_loss: 0.0136\n",
            "Epoch 396/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0136\n",
            "Epoch 397/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0119 - val_loss: 0.0136\n",
            "Epoch 398/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0119 - val_loss: 0.0136\n",
            "Epoch 399/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0119 - val_loss: 0.0136\n",
            "Epoch 400/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0136\n",
            "Epoch 401/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0136\n",
            "Epoch 402/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0119 - val_loss: 0.0135\n",
            "Epoch 403/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 404/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 405/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 406/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 407/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 408/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 409/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 410/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 411/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 412/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0118 - val_loss: 0.0135\n",
            "Epoch 413/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0118 - val_loss: 0.0134\n",
            "Epoch 414/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 415/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 416/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 417/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 418/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 419/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 420/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 421/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 422/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 423/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 424/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 425/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 426/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 427/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0117 - val_loss: 0.0134\n",
            "Epoch 428/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0116 - val_loss: 0.0134\n",
            "Epoch 429/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0134\n",
            "Epoch 430/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0134\n",
            "Epoch 431/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0134\n",
            "Epoch 432/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0134\n",
            "Epoch 433/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 434/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 435/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 436/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 437/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 438/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 439/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 440/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 441/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 442/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 443/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 444/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 445/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 446/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 447/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 448/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 449/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 450/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 451/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 452/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 453/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0116 - val_loss: 0.0133\n",
            "Epoch 454/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 455/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 456/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 457/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 458/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 459/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 460/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 461/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 462/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 463/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 464/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 465/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 466/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 467/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 468/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 469/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 470/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 471/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 472/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 473/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 474/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 475/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 476/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 477/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 478/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 479/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 480/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 481/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 482/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 483/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 484/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 485/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 486/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 487/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 488/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 489/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 490/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 491/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 492/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 493/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 494/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 495/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 496/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 497/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 498/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 499/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 500/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 501/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 502/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 503/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 504/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 505/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 506/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0115 - val_loss: 0.0132\n",
            "Epoch 507/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 508/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 509/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 510/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 511/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 512/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 513/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 514/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 515/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 516/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 517/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 518/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 519/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 520/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 521/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 522/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 523/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 524/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 525/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 526/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 527/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 528/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 529/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 530/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 531/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 532/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 533/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 534/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 535/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 536/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 537/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 538/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 539/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 540/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 541/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 542/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 543/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0114 - val_loss: 0.0132\n",
            "Epoch 544/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 545/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 546/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 547/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 548/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 549/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 550/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 551/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 552/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 553/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 554/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 555/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 556/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 557/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 558/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 559/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 560/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 561/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 562/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 563/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 564/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 565/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 566/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 567/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 568/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 569/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 570/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 571/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 572/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 573/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 574/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 575/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 576/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 577/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 578/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0114 - val_loss: 0.0131\n",
            "Epoch 579/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 580/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 581/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 582/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 583/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 584/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 585/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 586/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 587/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 588/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 589/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 590/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 591/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 592/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 593/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 594/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 595/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 596/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 597/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 598/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 599/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 600/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 601/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 602/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 603/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 604/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 605/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 606/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 607/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 608/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 609/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 610/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 611/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 612/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 613/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 614/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0131\n",
            "Epoch 615/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 616/1000\n",
            "2506/2506 [==============================] - 0s 21us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 617/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 618/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 619/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 620/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 621/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 622/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 623/1000\n",
            "2506/2506 [==============================] - 0s 20us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 624/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 625/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 626/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 627/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 628/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 629/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 630/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 631/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 632/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 633/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 634/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 635/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 636/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 637/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 638/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 639/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 640/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 641/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 642/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 643/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 644/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 645/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 646/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 647/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 648/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 649/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 650/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 651/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 652/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 653/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 654/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 655/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 656/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 657/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 658/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 659/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 660/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 661/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 662/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 663/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 664/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 665/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 666/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 667/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 668/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 669/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 670/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 671/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 672/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 673/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 674/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 675/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0130\n",
            "Epoch 676/1000\n",
            "2506/2506 [==============================] - 0s 20us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 677/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 678/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 679/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 680/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 681/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 682/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 683/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 684/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 685/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 686/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 687/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 688/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 689/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 690/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 691/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 692/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 693/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 694/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 695/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 696/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 697/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 698/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 699/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 700/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 701/1000\n",
            "2506/2506 [==============================] - 0s 20us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 702/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 703/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 704/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 705/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 706/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 707/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 708/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 709/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 710/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 711/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 712/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 713/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 714/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 715/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 716/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 717/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 718/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 719/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 720/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 721/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 722/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 723/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 724/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 725/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 726/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 727/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 728/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 729/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 730/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 731/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 732/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 733/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0129\n",
            "Epoch 734/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 735/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 736/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 737/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 738/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 739/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 740/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 741/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 742/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 743/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 744/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 745/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 746/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 747/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 748/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 749/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 750/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 751/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 752/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 753/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 754/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 755/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 756/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 757/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 758/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 759/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0111 - val_loss: 0.0128\n",
            "Epoch 760/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 761/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 762/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 763/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 764/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 765/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 766/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 767/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 768/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 769/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 770/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 771/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 772/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 773/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 774/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 775/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 776/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 777/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 778/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 779/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 780/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 781/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 782/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 783/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 784/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 785/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 786/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 787/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 788/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 789/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 790/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 791/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 792/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 793/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 794/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 795/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 796/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 797/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 798/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 799/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 800/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 801/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 802/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 803/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 804/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 805/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 806/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 807/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 808/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 809/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 810/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 811/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0110 - val_loss: 0.0127\n",
            "Epoch 812/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 813/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 814/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 815/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 816/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 817/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 818/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 819/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 820/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 821/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 822/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 823/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 824/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 825/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 826/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 827/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 828/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 829/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 830/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 831/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 832/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 833/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 834/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 835/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 836/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 837/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 838/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 839/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 840/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 841/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 842/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 843/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 844/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 845/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 846/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 847/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 848/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 849/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 850/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 851/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 852/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 853/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 854/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 855/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 856/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 857/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 858/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 859/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 860/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 861/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 862/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0109 - val_loss: 0.0126\n",
            "Epoch 863/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 864/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 865/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 866/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 867/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 868/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 869/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 870/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 871/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 872/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 873/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 874/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 875/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 876/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 877/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 878/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 879/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 880/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 881/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0108 - val_loss: 0.0126\n",
            "Epoch 882/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 883/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 884/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 885/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 886/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 887/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 888/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 889/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 890/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 891/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 892/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 893/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 894/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 895/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 896/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 897/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 898/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 899/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 900/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 901/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 902/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 903/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 904/1000\n",
            "2506/2506 [==============================] - 0s 20us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 905/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 906/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 907/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 908/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 909/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 910/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 911/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 912/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 913/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 914/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0108 - val_loss: 0.0125\n",
            "Epoch 915/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 916/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 917/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 918/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 919/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 920/1000\n",
            "2506/2506 [==============================] - 0s 20us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 921/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 922/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 923/1000\n",
            "2506/2506 [==============================] - 0s 23us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 924/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 925/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 926/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 927/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 928/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 929/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 930/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 931/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 932/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0125\n",
            "Epoch 933/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 934/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 935/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 936/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 937/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 938/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 939/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 940/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 941/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 942/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 943/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 944/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 945/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 946/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 947/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 948/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 949/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 950/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 951/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 952/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 953/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 954/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 955/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 956/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 957/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 958/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 959/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 960/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0107 - val_loss: 0.0124\n",
            "Epoch 961/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 962/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 963/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 964/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 965/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 966/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 967/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 968/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 969/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 970/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 971/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 972/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 973/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 974/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 975/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 976/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 977/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 978/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 979/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 980/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 981/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 982/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0106 - val_loss: 0.0124\n",
            "Epoch 983/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 984/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 985/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 986/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 987/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 988/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 989/1000\n",
            "2506/2506 [==============================] - 0s 17us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 990/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 991/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 992/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 993/1000\n",
            "2506/2506 [==============================] - 0s 18us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 994/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 995/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 996/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 997/1000\n",
            "2506/2506 [==============================] - 0s 15us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 998/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 999/1000\n",
            "2506/2506 [==============================] - 0s 19us/step - loss: 0.0106 - val_loss: 0.0123\n",
            "Epoch 1000/1000\n",
            "2506/2506 [==============================] - 0s 16us/step - loss: 0.0106 - val_loss: 0.0123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CebcKzSW_g6P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's visualize the training and validation losses."
      ]
    },
    {
      "metadata": {
        "id": "U9G1OHXoroBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "d0ba7ee2-92b9-4eda-87c0-11b40ce83449"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "for label in [\"loss\",\"val_loss\"]:\n",
        "    plt.plot(train_history.history[label], label=label)\n",
        "\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title(\"The final validation loss: {}\".format(train_history.history[\"val_loss\"][-1]))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAFMCAYAAACH0y5vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9c93e9b410>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Rs_-XnYhKl_N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's look at the parameters for the trained model."
      ]
    },
    {
      "metadata": {
        "id": "q4gtwBT7Kgh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "c212be9b-bd62-41cf-9163-33f69e306d95"
      },
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "  print(\"{}, {}\".format(layer.name, layer.get_weights()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_layer, [array([[0.7633411]], dtype=float32), array([[-0.56165993]], dtype=float32), array([0.04831408], dtype=float32)]\n",
            "output_layer, [array([[2.4384913]], dtype=float32), array([-0.06346191], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bqFBu_dCsUqi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**QUESTION**: \n",
        "* Relate the above weights to the terms in the equation for the vanilla RNN we saw earlier, namely:\n",
        "  * input-to-hidden $W_{xh}$,\n",
        "  * hidden-to-hidden $W_{hh}$,\n",
        "  * hidden-to-output weights $W_{hy}$\n",
        "  * recurrent and out biases."
      ]
    },
    {
      "metadata": {
        "id": "0FHaN-VXfxEl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Make predictions using the trained model"
      ]
    },
    {
      "metadata": {
        "id": "IQl_msx-4o3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "7c2ac8d5-231f-47af-d227-092510c058ff"
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test[:100])\n",
        "plt.figure(figsize=(19,3))\n",
        "\n",
        "plt.plot(y_test[:100], label=\"true\")\n",
        "plt.plot(y_pred, label=\"predicted\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABE0AAADCCAYAAAC8Njw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQQYhD/HyUXQgA7s75JAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9c91608110>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MQj5P-FKOgqG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**YOUR TASKS**: \n",
        "* [**ALL**] Change the learning rate and retrain. What happens when it is too large? What happens when it is too small?\n",
        "* [**ALL**] Change the `SimpleRNN` to `GRU`.\n",
        "  * What is the effect on the number of parameters? Can you explain why? Now do the same for `LSTM`.\n",
        "* [**INTERMEDIATE**] Note that the loss does not decrease much after around epoch 400. Add \"Early Stopping with patience\" to the `model.fit()` function to stop it from training beyond this point. **Hint**: Look at tf.keras.callbacks.\n",
        "  * *Early stopping* is a technique where we stop training the model once it's performance on validation data stops improving. Early stopping *with patience* means as soon as the model starts doing worse on validation we wait for at least `patience` more evaluations before stopping training, and if it improves within that time, we reset the counter. It's a way to avoid stopping too early.\n"
      ]
    },
    {
      "metadata": {
        "id": "_totIpxmZ8_v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Generating Shakespeare\n",
        "\n",
        "Now let's build an RNN language model to generate Shakespearian English! A language model is trained to assign high probabilities to sequences of words or sentences that are well formed, and low probabilities to sequences which are not realistic. When the model is trained, one can use it to *generate* data that is similar to the training data.\n",
        "\n",
        "Our data is now sequences of discrete symbols (characters). But neural networks operate in continuous spaces, and so we need to take the discrete language data, and **embed** it in a continuous space. To do this, we'll simply break up the data into sequences of characters, and represent each character using a learned vector. This is a standard trick for processing text using neural networks. "
      ]
    },
    {
      "metadata": {
        "id": "9dFQtnDLZa3c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download and Preprocess the Data"
      ]
    },
    {
      "metadata": {
        "id": "Tmmjc-EigwCw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We first download the data and examine what it looks like:"
      ]
    },
    {
      "metadata": {
        "id": "hybIopOLPD4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "eded9c5c-2ece-4774-b207-8990b931f782"
      },
      "cell_type": "code",
      "source": [
        "context = ssl._create_unverified_context()\n",
        "shakespeare_url = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\n",
        "\n",
        "data = urllib2.urlopen(shakespeare_url, context=context)\n",
        "all_text = data.read().lower()\n",
        "\n",
        "print(\"Downloaded Shakespeare data with {} characters.\".format(len(all_text)))\n",
        "print(\"FIRST 1000 CHARACTERS: \")\n",
        "print(all_text[:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded Shakespeare data with 4573338 characters.\n",
            "FIRST 1000 CHARACTERS: \n",
            "first citizen:\n",
            "before we proceed any further, hear me speak.\n",
            "\n",
            "all:\n",
            "speak, speak.\n",
            "\n",
            "first citizen:\n",
            "you are all resolved rather to die than to famish?\n",
            "\n",
            "all:\n",
            "resolved. resolved.\n",
            "\n",
            "first citizen:\n",
            "first, you know caius marcius is chief enemy to the people.\n",
            "\n",
            "all:\n",
            "we know't, we know't.\n",
            "\n",
            "first citizen:\n",
            "let us kill him, and we'll have corn at our own price.\n",
            "is't a verdict?\n",
            "\n",
            "all:\n",
            "no more talking on't; let it be done: away, away!\n",
            "\n",
            "second citizen:\n",
            "one word, good citizens.\n",
            "\n",
            "first citizen:\n",
            "we are accounted poor citizens, the patricians good.\n",
            "what authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know i\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jIuagNcdQLqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_text = all_text[:1000000] # Keep only the first 1 million characters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UK1x69AngvLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now preprocess the text data as follows:\n",
        "\n",
        "1. Extract the vocabulary of all `vocab_size` unique characters appearing in the data.\n",
        "\n",
        "2. Assign each character a unique integer id in `0 <= id < vocab_size`. This is so we can map the characters to unique embedding vectors.\n",
        "\n",
        "3. Split the data into sequences (\"windows\") of `max_len` characters (the input to the model) followed by the next character as target. E.g. using `max_len=5` the sentence \"I saw a cat\" (11 characters) will get split into \"I saw\" and /space/, \"/space/saw/space/\" and \"a\", \"saw a\" and /space/, etc. To add some variation, we skip `step` characters between each sequence (i.e. we use a \"sliding window of `max_len` with stride `step`\")."
      ]
    },
    {
      "metadata": {
        "id": "zPvsNXKZPJKz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "1dcaaa7f-b946-4f8d-cb1b-32674b125465"
      },
      "cell_type": "code",
      "source": [
        "max_len = 30  # We only consider this many previous data points (characters)\n",
        "step = 3 # We start a new training sequence every `step` characters\n",
        "sentences = [] # This holds our extracted sequences\n",
        "next_chars = [] # This holds the targets (the follow-up characters)\n",
        "\n",
        "chars = sorted(list(set(training_text)))  # List of unique characters in the corpus\n",
        "vocab_size = len(chars)\n",
        "print('Number of unique characters: ', vocab_size)\n",
        "print(chars)\n",
        "\n",
        "# Construct dictionaries mapping unique characters to their index in `chars` and reverse\n",
        "char2index = dict((c, chars.index(c)) for c in chars)\n",
        "index2char = dict((chars.index(c), c) for c in chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique characters:  39\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O2PhcQwrc2JX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we encode the training data by mapping each character to its unique integer id."
      ]
    },
    {
      "metadata": {
        "id": "xY0qXZmVq8kW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89446005-6eba-42da-fce5-8bcfb723a567"
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, len(training_text) - max_len, step):\n",
        "    sentences.append([char2index[s] for s in training_text[i: i + max_len]])\n",
        "    next_chars.append([char2index[s] for s in training_text[i + max_len]])\n",
        "\n",
        "print('Number of extracted sequences:', len(sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of extracted sequences: 333324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PsZy6Kshc-Sp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This yields the following numpy arrays:"
      ]
    },
    {
      "metadata": {
        "id": "vhgSaP5ntDtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2af89a0d-e7e3-4744-8d8d-aa9ec1ab9293"
      },
      "cell_type": "code",
      "source": [
        "X, Y = np.array(sentences, dtype=np.int64), np.array(next_chars, dtype=np.int64)\n",
        "X.shape, Y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((333324, 30), (333324, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "sBR2LsXjmqTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the first example."
      ]
    },
    {
      "metadata": {
        "id": "2H1jobcrmwK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "3826ee10-ca7f-466f-c0f4-5ab37ef7aab7"
      },
      "cell_type": "code",
      "source": [
        "print(\"X[0].shape = {}, Y[0].shape = {}\".format(X[0].shape, Y[0].shape))\n",
        "print(\"X[0]: \", X[0])\n",
        "print(\"Y[0]: \", Y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X[0].shape = (30,), Y[0].shape = (1,)\n",
            "X[0]:  [18 21 30 31 32  1 15 21 32 21 38 17 26 10  0 14 17 18 27 30 17  1 35 17\n",
            "  1 28 30 27 15 17]\n",
            "Y[0]:  [17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tEQWEZlSZgpF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Build an RNN language model"
      ]
    },
    {
      "metadata": {
        "id": "rCysOLWadPHF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A **language model** estimates a probability distribution over sequences $\\mathbb{x}_{1:N} = (x_1, x_2, ..., x_N)$ by breaking up the full joint probability into a sequence of conditional probabilities using the **[chain-rule of probability](https://en.wikipedia.org/wiki/Chain_rule_(probability))**:\n",
        "\n",
        "\\begin{align}\n",
        " p(\\mathbb{x}_{1:N}) &= p(x_1) \\cdot p(x_2 | x_1) \\cdot p(x_3 | x_2, x_1) \\ldots \\\\\n",
        " &= \\Pi_1^N p(x_i | \\mathbb{x}_{1:i-1})\n",
        "\\end{align}\n",
        "\n",
        "In other words, to model the probability of the phrase \"*i saw a cat*\" at the character level, the model learns to estimate the probabilities for p(i), p(/space/| i), p(c | i, /space/), and so forth, and multiplies them together. \n",
        "\n",
        "There are many different ways in which to estimate these individual probabilities. But one particularly effective way is to use an RNN! To do this, we'll therefore be modeling the  $p(x_i | \\mathbb{x}_{1:i-1})$ terms using an RNN conditioned on $\\mathbb{x}_{1:i-1}$.\n",
        "\n",
        "* We model these probabilities at the character-level, so we'll use an `Embedding` layer as the first layer of our model to map the discrete character id's to real-valued embedding vectors. \n",
        "* Next, the RNN-core will map these sequences of character embeddings to a probability distribution over all characters $p(x_i | \\mathbb{x}_{1:i-1}) \\in \\mathbb{R}^\\textrm{vocab_size}$ at every step of the sequence. To do this, the RNN will map the embeddings to a sequence of *hidden states*. We will then use a `Dense` layer to map from the RNN hidden state to an output distribution over the total number of characters using a [`softmax`](https://en.wikipedia.org/wiki/Softmax_function) activation.\n",
        "\n",
        "We can do this with a few lines of code:"
      ]
    },
    {
      "metadata": {
        "id": "k8AxhQuePOCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "35b12bba-5085-487e-8f08-83d149261ca1"
      },
      "cell_type": "code",
      "source": [
        "embedding_dim = 32   # Map each character to a unique vector of this dimension\n",
        "vocab_size = len(chars)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(\n",
        "  vocab_size, embedding_dim, \n",
        "  input_length=max_len, \n",
        "  embeddings_initializer=tf.keras.initializers.TruncatedNormal))\n",
        "model.add(tf.keras.layers.LSTM(\n",
        "  128, \n",
        "  input_shape=(max_len, embedding_dim),  # NB: Ensure this matches the embedding_dim!\n",
        "  dropout=0.1,  # input-to-hidden drop-probability\n",
        "  recurrent_dropout=0.2))  # hidden-to-hidden drop-probability\n",
        "model.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 30, 32)            1248      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               82432     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 39)                5031      \n",
            "=================================================================\n",
            "Total params: 88,711\n",
            "Trainable params: 88,711\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m4fFjFtZokf2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Select the optimizer and loss"
      ]
    },
    {
      "metadata": {
        "id": "RHOWm55whZUt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once we have a model that can map sequence of characters to a probability distribution over the next character in the sequence, we can train it using **maximum likelihood** on the training set to find the model parameters which maximizes the probability of the training data. Again, this is very simple to do by choosing an optimizer and selecting the `sparse_categorical_crossentropy` loss function:"
      ]
    },
    {
      "metadata": {
        "id": "hKJNZXs7PSW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "loss='sparse_categorical_crossentropy'\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dEr9hEqFiGx6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Helper functions"
      ]
    },
    {
      "metadata": {
        "id": "3zXa95pBPUw3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_with_temp(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyH6U3er5fis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c7d6e470-604f-4b85-e6bd-433f88e0069e"
      },
      "cell_type": "code",
      "source": [
        "def shift_and_append(test_arr, next_item):\n",
        "  '''Returns a copy of test_arr with items shifted one position to the left and \n",
        "     next_item appended.\n",
        "  '''\n",
        "  tmp = np.empty_like(test_arr)\n",
        "  tmp[:,:-1] = test_arr[:,1:]\n",
        "  tmp[:,-1] = next_item\n",
        "  return tmp\n",
        "\n",
        "## TEST the above function:\n",
        "test_arr = np.array([[1,2,3,4]])\n",
        "\n",
        "print(\"test_arr = {}\".format(test_arr))\n",
        "test_arr = shift_and_append(test_arr, 5)\n",
        "print(\"roll_arr(test_arr, 5) = {}\".format(test_arr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_arr = [[1 2 3 4]]\n",
            "roll_arr(test_arr, 5) = [[2 3 4 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q9le2p0YxeYD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_from_model(model, \n",
        "                      num_generate=400, \n",
        "                      prev_text=None,  # the text used to condition the model\n",
        "                      temperatures=[0.2, 0.5, 1.0, 1.2]):\n",
        "    \n",
        "    if not prev_text:\n",
        "      # Select a text seed at random\n",
        "      start_index = random.randint(0, len(training_text) - max_len - 1)\n",
        "      while ((start_index < (len(training_text) - max_len - 1)) and (\n",
        "        training_text[start_index - 1] is not ' ')):\n",
        "        start_index += 1  # Advance to beginning of new word\n",
        "      prev_text = training_text[start_index: start_index + max_len]\n",
        "    \n",
        "    if len(prev_text) != max_len:\n",
        "      print(\"`prev_text` must be of length `max_len`.\")\n",
        "      return\n",
        "\n",
        "    print('GENERATING TEXT WITH SEED: \\n\"' + prev_text + '\"')\n",
        "    prev_text_arr = np.array(\n",
        "      [[char2index[c] for c in prev_text]], dtype=np.int64) \n",
        "    \n",
        "    for temp in temperatures:\n",
        "        print('==TEMPERATURE:', temp)\n",
        "        sys.stdout.write(prev_text)\n",
        "\n",
        "        # Start with the same sampled text for all temperatures\n",
        "        generated_text = prev_text \n",
        "        generated_text_arr = prev_text_arr\n",
        "\n",
        "        # Now generate this many characters\n",
        "        for i in range(num_generate):         \n",
        "            \n",
        "            # Get the output softmax given the conditioning text\n",
        "            #prev_text = generated_text_enc[np.newaxis,:]\n",
        "            preds = model.predict(generated_text_arr, verbose=0)[0]\n",
        "            \n",
        "            next_index = sample_with_temp(preds, temp)\n",
        "            next_char = index2char[next_index]\n",
        "            generated_text += next_char\n",
        "            generated_text = generated_text[1:]\n",
        "\n",
        "            # Left-shift and add into encoded array\n",
        "            generated_text_arr = shift_and_append(generated_text_arr, next_index)\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a78CrsaMiKrA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Train the model\n",
        "\n",
        "Let's train the model! The code below will train the model on a subset of the available data, and then generate from the model every `sample_every` number of batches.\n",
        "\n",
        "To generate from the model, we use `model.predict()` on a sequence of `max_len` conditioning characters to produce an output distribution over `vocab_size` characters. We then sample one character from this distribution and shift everything up by one and append the new characters. By repeating this, we can generate text from the (partially-trained) model.\n",
        "\n",
        "**NOTE**: \n",
        "* It takes a while to train a model that starts generating anything resembling the Shapespeare text! In general it should start getting the rough structure in place around the 100K training example mark (examples, not batches). But to generate any meaningful words will need several hundred thousand examples.\n",
        "* We sample with *temperature*. This is a way to sharpen or flatten the probabilities produced by the model. By lowering the temperature, we emphasize the modes of the predicted distribution, and by increasing the temperature, we flatten the modes (tends towards uniform)."
      ]
    },
    {
      "metadata": {
        "id": "0RcFKgkdPXuI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "total_num_batches = X.shape[0] // batch_size\n",
        "sample_every = 256  # Train on this many batches, then generate something\n",
        "\n",
        "print(\"Training on {} batches in total.\".format(total_num_batches))\n",
        "\n",
        "for cur_batch in range(0, total_num_batches, sample_every):\n",
        "    print('TRAINING ON BATCH {} to {} (example {} to {})'.format(\n",
        "      cur_batch, cur_batch + sample_every,\n",
        "      cur_batch * batch_size, (cur_batch + sample_every) * batch_size)\n",
        "    )\n",
        "    \n",
        "    X_batch = X[batch_size * cur_batch : batch_size * (cur_batch + sample_every), :]\n",
        "    Y_batch = Y[batch_size * cur_batch : batch_size * (cur_batch + sample_every), :]\n",
        "    \n",
        "    '''\n",
        "    # Show the first 5 examples to make sure we're not training on garbage\n",
        "    print(\"X_batch.shape = {}\".format(X_batch.shape))\n",
        "    print(\"Y_batch.shape = {}\".format(Y_batch.shape))\n",
        "    print(\"FIRST 5 EXAMPLES:\")\n",
        "    for num in range(5):\n",
        "      in_seq = [index2char[int(indx)] for indx in np.nditer(X_batch[num, :])]\n",
        "      next_char = index2char[Y_batch[num, 0]]\n",
        "      print(str(num) + '. ' + ''.join(in_seq) + '-->' + next_char)\n",
        "    '''\n",
        "       \n",
        "    model.fit(X_batch, Y_batch,\n",
        "              batch_size=batch_size,\n",
        "              epochs=1,\n",
        "              verbose=1)\n",
        "\n",
        "    print(\"GENERATING SOME RANDOM TEXT FROM THE MODEL\")\n",
        "    sample_from_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3C-AIU18HFe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE**: Even after training has stopped you can still generate from the (partially trained) model as follows:"
      ]
    },
    {
      "metadata": {
        "id": "7HR4JXD3ZnX0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82fbdad8-0586-4096-f5a1-7c1cc9bfcee4"
      },
      "cell_type": "code",
      "source": [
        "my_text = \"       the meaning of life is:\"  # Needs to be max_len characters\n",
        "print(len(my_text))\n",
        "sample_from_model(model, prev_text=my_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UbdLbn_xRe21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###IMPORTANT NOTES\n",
        "* Even if you stop training the model weights are persistant. If you resume training it will start where you left off. \n",
        "* To reset the weights, you need to recompile the model.\n",
        "* Sampling is **stochastic** (random), so you'll get new outputs every time you rerun the sampling code.\n",
        "\n",
        "### YOUR TASKS: \n",
        "* [**ALL**] Read the generations from your model in a funny voice to your neighbour.\n",
        "* [**ALL**] Increase `max_len` and regenerate the data and retrain the model.\n",
        " * What's the effect on training speed as you double `max_len`. Can you explain why?\n",
        " * Do you notice any effect on the quality of the model? Can you explain why?\n",
        "* [**ALL**] Change `embedding_dim` and the hidden size of the LSTM and observe the effect on training speed and quality.\n",
        "* [**INTERMEDIATE**] Add dropout to the model. What types of dropout do we get for recurrent models? What's the effect on the text quality?\n"
      ]
    }
  ]
}